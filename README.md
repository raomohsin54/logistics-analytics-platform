# ğŸ“¦ Logistics Analytics Platform â€“ End-to-End Azure Data Engineering Project

This project demonstrates a modern data engineering workflow using the **Medallion Architecture (Bronze â†’ Silver â†’ Gold)** to deliver operational logistics insights.  
It's designed for enterprise-scale logistics or transport companies to monitor **shipment performance**, **delays**, and **fleet/vendor KPIs** using:

- ğŸš› **Azure Data Factory**  
- ğŸš€ **Azure Databricks + PySpark**  
- ğŸ“ **Delta Lake + Parquet**  
- ğŸ“Š **Power BI for Visualization**

---

## ğŸ§± Architecture

![Architecture Diagram](architecture/Architecture%20Diagram.png)

---

## ğŸ“ Folder Structure

logistics-analytics-platform/
â”‚
â”œâ”€â”€ data/ # Dummy CSVs (drivers, vendors, routes, shipments)
â”œâ”€â”€ notebooks/ # Databricks Notebooks for each layer
â”œâ”€â”€ adf_pipelines/ # ADF JSON definitions
â”œâ”€â”€ powerbi/ # Power BI screenshots or .pbix files
â”œâ”€â”€ architecture/ # Architecture diagram image
â”œâ”€â”€ README.md # This file
â””â”€â”€ .gitignore

---

## ğŸ“Š Power BI Dashboard Features

- âœ… KPI Cards: Total Shipments, Avg Delay, On-Time %
- ğŸ“Š Vendor performance bar charts
- ğŸ“ˆ Monthly delivery trend lines
- ğŸ—ºï¸ Route-level delay matrix (simulated map)
- ğŸ¯ Filters: Vendor, Route Type, Origin, Destination

---

## ğŸ“Œ Pipeline Stages

### ğŸ”¹ Bronze Layer (Raw Ingestion)
- Raw files: `drivers.csv`, `vendors.csv`, `routes.csv`, `shipments.csv`
- Ingested using **ADF Copy Activity** with **ForEach loop**
- Stored in Azure Data Lake under the `bronze/` container

### ğŸ”¸ Silver Layer (Cleansed)
- Field renaming and schema standardization
- Converted timestamps and metrics (e.g., delay in minutes)
- Stored as partitioned Parquet files in `silver/` container

### ğŸŸ¡ Gold Layer (Aggregated)
- Enriched metrics like:
  - On-Time %
  - Delay by route
  - Vendor KPIs
  - Monthly trends
- Written to `gold/` container, ready for Power BI

---

## ğŸ§  Skills Demonstrated

- âœ… Metadata-driven ADF pipelines (parameterized + looped)
- ğŸ§  Databricks data transformation using PySpark
- ğŸ—ƒï¸ Medallion architecture (Bronze â†’ Silver â†’ Gold)
- ğŸ’¾ Delta Lake & partitioned Parquet files
- ğŸ“… Incremental + batch pipeline logic
- ğŸ“ˆ Clean and professional Power BI dashboards
- ğŸ” Secure handling of storage & access configuration

---

## ğŸ“¤ How to Reproduce

1. Clone this repo
2. Upload `/data/` CSVs to Azure Data Lake Gen2 (`raw` container)
3. Import and run ADF pipelines from `/adf_pipelines/`
4. Execute transformation notebooks in `/notebooks/` inside Databricks
5. Open Power BI file from `/powerbi/` and connect to `gold` container

---

## ğŸ“© Contact

ğŸ“§ Email: **rao.mohsin.54@gmail.com**  
ğŸŒ [LinkedIn](https://www.linkedin.com/in/mohsin-mukhtiar/)  
âœï¸ [Medium Profile](https://medium.com/@rao.mohsin.54)

---

## â­ Star This Repo

If you found this project useful, give it a â­ on GitHub â€” and feel free to fork or adapt it!

---
